{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c16de3-8a53-4892-ba92-918f0ea8897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U psycopg2-binary\n",
    "# !pip install -U jsondiff\n",
    "# !pip install -U deepdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb42df7c-c6c4-441e-9ef8-6bae81375979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "# import jsondiff\n",
    "from deepdiff import DeepDiff  # https://pypi.org/project/deepdiff/\n",
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "from datetime import date, datetime, timezone\n",
    "import csv\n",
    "import time\n",
    "\n",
    "with open('./config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "db_file = './app/patron_validation_data.db'\n",
    "\n",
    "class SQLiteConnection:\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "    with SQLiteConnection('your_database_path.sqlite') as conn:\n",
    "        # database operations ...\n",
    "        pass\n",
    "    \"\"\"\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_diff(json_old, json_new):\n",
    "        try:\n",
    "            # Example using DeepDiff\n",
    "            # json_old = json.dumps(\n",
    "            #     [\n",
    "            #         {\n",
    "            #             \"address_type_code\": \"a\",\n",
    "            #             \"addr1\": \"123 Fake St\",\n",
    "            #             \"city\": \"Springfield\",\n",
    "            #             \"postal_code\": \"45401\",\n",
    "            #             \"region\": \"OH\"\n",
    "            #         }, \n",
    "            #         {\n",
    "            #             \"address_type_code\": \"a\",\n",
    "            #             \"addr1\": None,\n",
    "            #             \"city\": None,\n",
    "            #             \"postal_code\": None,\n",
    "            #             \"region\": None         \n",
    "            #         }\n",
    "            #     ]\n",
    "            # )\n",
    "            # json_new = json.dumps(\n",
    "            #     [\n",
    "            #         {\n",
    "            #             \"address_type_code\": \"a\",\n",
    "            #             \"addr1\": \"742 Evergreen Terrace\",\n",
    "            #             \"city\": \"Springfield\",\n",
    "            #             \"postal_code\": \"45501\",\n",
    "            #             \"region\": \"OH\"\n",
    "            #         }\n",
    "            #     ]\n",
    "            # )\n",
    "            # print(\n",
    "            #     DeepDiff(\n",
    "            #         json.loads(json_old), # we expect json coming from the database to be in a string\n",
    "            #         json.loads(json_new), # \"\"\n",
    "            #         ignore_order=True, \n",
    "            #         verbose_level=2\n",
    "            #     ).to_json()\n",
    "            # )\n",
    "            # output looks like this ...\n",
    "            # {\n",
    "            #   \"values_changed\": {\n",
    "            #     \"root[0]\": {\n",
    "            #       \"new_value\": {\n",
    "            #         \"address_type_code\": \"a\",\n",
    "            #         \"addr1\": \"742 Evergreen Terrace\",\n",
    "            #         \"city\": \"Springfield\",\n",
    "            #         \"postal_code\": \"45501\",\n",
    "            #         \"region\": \"OH\"\n",
    "            #       },\n",
    "            #       \"old_value\": {\n",
    "            #         \"address_type_code\": \"a\",\n",
    "            #         \"addr1\": \"123 Fake St\",\n",
    "            #         \"city\": \"Springfield\",\n",
    "            #         \"postal_code\": \"45401\",\n",
    "            #         \"region\": \"OH\"\n",
    "            #       }\n",
    "            #     }\n",
    "            #   },\n",
    "            #   \"iterable_item_removed\": {\n",
    "            #     \"root[1]\": {\n",
    "            #       \"address_type_code\": \"a\",\n",
    "            #       \"addr1\": null,\n",
    "            #       \"city\": null,\n",
    "            #       \"postal_code\": null,\n",
    "            #       \"region\": null\n",
    "            #     }\n",
    "            #   }\n",
    "            # }\n",
    "\n",
    "            diff_results = DeepDiff(\n",
    "                json.loads(json_old), # we expect json coming from the database to be in a string\n",
    "                json.loads(json_new), # ... but this module expects a \"dictionary, list, string or any python object\"\n",
    "                ignore_order=False,   # order should matter, since the array order represents display order \n",
    "                verbose_level=2\n",
    "            ).to_json()\n",
    "            \n",
    "            # return an empty json array if the result of the diff is None, \n",
    "            # ... or the json result of the diff \n",
    "            return str(diff_results) if diff_results is not None else None\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error in compute_diff():', e) \n",
    "            return None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "        self.conn.create_function('json_diff', 2, self.compute_diff)\n",
    "        # run this sql script when the connection to the database is opened ... \n",
    "        sql = \"\"\"\\\n",
    "        PRAGMA locking_mode=SHARED;    -- https://www.sqlite.org/lockingv3.html\n",
    "        PRAGMA cache_size = 12800;     -- results in about 50MB of cache at the default 4KB page size\n",
    "        -- PRAGMA foreign_keys = ON;   -- TODO consider turning this on, and then running a check ...       \n",
    "        -- run a check ...\n",
    "        -- PRAGMA foreign_key_check;\n",
    "        \"\"\"\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.executescript(sql)\n",
    "        self.cursor.close()\n",
    "        del(self.cursor)\n",
    "        return self.conn\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # https://www.sqlite.org/pragma.html#pragma_analysis_limit\n",
    "        sql = \"\"\"\\\n",
    "        PRAGMA analysis_limit=-1;   -- no analysis limit\n",
    "        PRAGMA optimize;            -- analyze\n",
    "        PRAGMA analysis_limit=2000; -- set analysis limit back to something more reasonable\n",
    "        \"\"\"\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.executescript(sql)\n",
    "        \n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c3c594-3035-4443-bda2-e88b60a262a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up table schemas, indexes, and triggers\n",
    "with SQLiteConnection(db_file) as con:\n",
    "    cursor = con.cursor()\n",
    "\n",
    "    with open('./sql_schema.sql') as f:\n",
    "        cursor.executescript(f.read())\n",
    "\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9074583-aab1-48ef-8292-2f9a488dd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_patron_data(con, data, columns):\n",
    "    \"\"\"\n",
    "    takes the patron data returned from the Sierra SQL query, and inserts it into the local database\n",
    "    \n",
    "    con            : sqlite connection,\n",
    "    data           : data from the sierra result query\n",
    "    columns        : the column names\n",
    "    \"\"\"\n",
    "\n",
    "    sql_patron_insert = \"\"\"\\\n",
    "    INSERT INTO patrons (\n",
    "        patron_record_id, \n",
    "        patron_record_num, \n",
    "        campus_code, \n",
    "        barcode1, \n",
    "        home_library_code,\n",
    "        ptype_code, \n",
    "        create_timestamp_utc, \n",
    "        delete_timestamp_utc, \n",
    "        update_timestamp_utc, \n",
    "        expire_timestamp_utc, \n",
    "        active_timestamp_utc,\n",
    "        claims_returned_total, \n",
    "        owed_amt_cents, \n",
    "        mblock_code, \n",
    "        highest_level_overdue_num, \n",
    "        num_revisions\n",
    "    ) \n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ON CONFLICT(patron_record_id) DO UPDATE SET\n",
    "        patron_record_num = excluded.patron_record_num,\n",
    "        campus_code = excluded.campus_code,\n",
    "        barcode1 = excluded.barcode1,\n",
    "        home_library_code = excluded.home_library_code,\n",
    "        ptype_code = excluded.ptype_code,\n",
    "        create_timestamp_utc = excluded.create_timestamp_utc,\n",
    "        delete_timestamp_utc = excluded.delete_timestamp_utc,\n",
    "        update_timestamp_utc = excluded.update_timestamp_utc,\n",
    "        expire_timestamp_utc = excluded.expire_timestamp_utc,\n",
    "        active_timestamp_utc = excluded.active_timestamp_utc,\n",
    "        claims_returned_total = excluded.claims_returned_total,\n",
    "        owed_amt_cents = excluded.owed_amt_cents,\n",
    "        mblock_code = excluded.mblock_code,\n",
    "        highest_level_overdue_num = excluded.highest_level_overdue_num,\n",
    "        num_revisions = excluded.num_revisions\n",
    "    ;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        con.executemany(\n",
    "            sql_patron_insert, \n",
    "            (\n",
    "                (\n",
    "                    int(row[columns.index('patron_record_id')]) \\\n",
    "                        if row[columns.index('patron_record_id')] is not None else None,\n",
    "                    int(row[columns.index('patron_record_num')]) \\\n",
    "                        if row[columns.index('patron_record_num')] is not None else None,\n",
    "                    str(row[columns.index('campus_code')]) \\\n",
    "                        if row[columns.index('campus_code')] is not None else None,\n",
    "                    str(row[columns.index('barcode1')]) \\\n",
    "                        if row[columns.index('barcode1')] is not None else None,\n",
    "                    str(row[columns.index('home_library_code')]) \\\n",
    "                        if row[columns.index('home_library_code')] is not None else None,\n",
    "                    str(row[columns.index('ptype_code')]) \\\n",
    "                        if row[columns.index('ptype_code')] is not None else None,\n",
    "                    int(row[columns.index('create_timestamp_utc')]) \\\n",
    "                        if row[columns.index('create_timestamp_utc')] is not None else None,\n",
    "                    int(row[columns.index('delete_timestamp_utc')]) \\\n",
    "                        if row[columns.index('delete_timestamp_utc')] is not None else None,\n",
    "                    int(row[columns.index('update_timestamp_utc')]) \\\n",
    "                        if row[columns.index('update_timestamp_utc')] is not None else None,\n",
    "                    int(row[columns.index('expire_timestamp_utc')]) \\\n",
    "                        if row[columns.index('expire_timestamp_utc')] is not None else None,\n",
    "                    int(row[columns.index('active_timestamp_utc')]) \\\n",
    "                        if row[columns.index('active_timestamp_utc')] is not None else None,\n",
    "                    int(row[columns.index('claims_returned_total')]) \\\n",
    "                        if row[columns.index('claims_returned_total')] is not None else None,\n",
    "                    int(row[columns.index('owed_amt_cents')]) \\\n",
    "                        if row[columns.index('owed_amt_cents')] is not None else None,\n",
    "                    str(row[columns.index('mblock_code')]) \\\n",
    "                        if row[columns.index('mblock_code')] is not None else None,\n",
    "                    int(row[columns.index('highest_level_overdue_num')]) \\\n",
    "                        if row[columns.index('highest_level_overdue_num')] is not None else None,\n",
    "                    int(row[columns.index('num_revisions')]) \\\n",
    "                        if row[columns.index('num_revisions')] is not None else None,\n",
    "                )\n",
    "                for row in data\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# insert json_data function ... \n",
    "def insert_json_data(con, data, columns, json_data_type):\n",
    "    \"\"\"\n",
    "    con            : sqlite connection,\n",
    "    data           : data from the sierra result query\n",
    "    columns        : the column names\n",
    "    json_data_type : the type of data\n",
    "    ---\n",
    "    json_data_type come from the Sierra SQL results (columns)\n",
    "        patron_address_json,\n",
    "        identifiers_json,\n",
    "        phone_numbers_json,\n",
    "        emails_json\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    INSERT INTO patron_json_data (\n",
    "        patron_record_id,\n",
    "        json_data_type,\n",
    "        json_data\n",
    "    ) VALUES (\n",
    "        ?,\n",
    "        ?,\n",
    "        CASE WHEN json_valid(?) THEN ? ELSE NULL END\n",
    "    )\n",
    "    ON CONFLICT(patron_record_id, json_data_type) DO UPDATE SET\n",
    "        json_data = CASE WHEN json_valid(excluded.json_data) THEN excluded.json_data ELSE NULL END,\n",
    "        update_timestamp_utc = CASE \n",
    "            WHEN patron_json_data.json_data != excluded.json_data THEN strftime('%s', 'now')\n",
    "            ELSE patron_json_data.update_timestamp_utc\n",
    "        END\n",
    "    ;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        con.executemany(\n",
    "            sql,\n",
    "            (\n",
    "                (\n",
    "                    int(row[columns.index('patron_record_id')]),  # patron_record_id\n",
    "                    str(json_data_type),                          # json_data_type\n",
    "                    str(row[columns.index(json_data_type)]),      # json_data\n",
    "                    str(row[columns.index(json_data_type)]),      # json_data\n",
    "                    # NOTE, we need to send the json_data 2x since it's referenced 2x in the statement\n",
    "                )\n",
    "                for row in data\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'{json_data_type} Error:', e)\n",
    "\n",
    "\n",
    "def get_latest_update(cursor):\n",
    "    \"\"\"\n",
    "    extract the latest UNIX timestamp from the sqlite database\n",
    "    \"\"\"\n",
    "    \n",
    "    latest_update = 0 # 0 is the start time of UNIX EPOCH\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    SELECT\n",
    "        -- grab the last recent change we made\n",
    "        -- (assumes that our system clocks are in sync)\n",
    "        max(patrons.update_timestamp_utc)\n",
    "    FROM\n",
    "    \tpatrons\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        result = cursor_sqlite.fetchone()\n",
    "        if (\n",
    "            len(result) == 1\n",
    "            and result[0] is not None\n",
    "        ):\n",
    "            latest_update = str(result[0])\n",
    "\n",
    "        # print(latest_update, '\\n')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return(0)\n",
    "\n",
    "    return(latest_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1103b0c-3646-41de-b428-5a44e2884ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,.................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................1692106077,..1692106766,.1692106774,.1692106783,.1692106790,.1692106798,.1692106806,.1692106813,.1692106822,.1692106829,.1692106836,.1692106846,.1692106854,.1692106861,.1692106870,.1692106878,.1692106886,.1692106893,.1692106902,.1692106910,.1692106918,.1692106926,.1692106934,.1692106942,.1692106950,.1692106957,.1692106966,.1692106974,.1692106981,.1692106989,.1692106997,.1692107005,.1692107013,.1692107022,.1692107029,.1692107037,.1692107045,.1692107053,.1692107061,.1692107069,.1692107077,.1692107084,.1692107093,.1692107100,.1692107109,.1692107117,.1692107125,.1692107133,.1692107141,.1692107149,.1692107157,.1692107165,.1692107173,.1692107181,.1692107188,.1692107197,.1692107205,.1692107212,.1692107220,.1692107228,.1692107237,."
     ]
    }
   ],
   "source": [
    "# connect to both the sierra db, and the local sqlite db\n",
    "with psycopg2.connect(dsn=config['dsn']) as con, \\\n",
    "SQLiteConnection(db_file) as con_sqlite:\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + 60*60  # 60 minutes * 60 seconds/minute\n",
    "    # end_time = start_time + 5*60\n",
    "\n",
    "    while time.time() < end_time:\n",
    "        cursor = con.cursor(name=\"named_cursor\")\n",
    "        cursor_sqlite = con_sqlite.cursor()\n",
    "    \n",
    "        # get the latest UNIX timestamp from the local database\n",
    "        latest_update = get_latest_update(cursor_sqlite)\n",
    "        print(latest_update, end=',')\n",
    "    \n",
    "        try:\n",
    "            with open('./sierra_patron_data.sql') as f:\n",
    "                cursor.execute(f.read(), (latest_update,))\n",
    "        except Exception as e:\n",
    "            print('Error executing sierra_patron_data.sql', e)\n",
    "            \n",
    "        i = 0\n",
    "        try:\n",
    "            while(data:=cursor.fetchmany(1000)):\n",
    "                # get the columns from the cursor\n",
    "                columns = [col[0] for col in cursor.description]\n",
    "                \n",
    "                # insert the appropriate data into each local sqlite table ..\n",
    "                # patrons table\n",
    "                insert_patron_data(con_sqlite, data, columns)\n",
    "    \n",
    "                # insert the json data\n",
    "                # (NOTE: `json_data_type` is the row's column data from the Sierra \n",
    "                # SQL query that contains the JSON data of that type \n",
    "                insert_json_data(con_sqlite, data, columns, 'patron_address_json')\n",
    "                insert_json_data(con_sqlite, data, columns, 'identifiers_json')\n",
    "                insert_json_data(con_sqlite, data, columns, 'phone_numbers_json')\n",
    "                insert_json_data(con_sqlite, data, columns, 'emails_json')\n",
    "                \n",
    "                i+=1\n",
    "                print('.', end='')\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "              \n",
    "        # print('done. ', i)\n",
    "        \n",
    "        con_sqlite.commit()  # move inserted data to the database\n",
    "\n",
    "        # cleanup \n",
    "        cursor_sqlite.close()\n",
    "        cursor.close()\n",
    "        del(cursor_sqlite)\n",
    "        del(cursor)\n",
    "        \n",
    "        time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
