{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c16de3-8a53-4892-ba92-918f0ea8897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U psycopg2-binary\n",
    "# !pip install -U jsondiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb42df7c-c6c4-441e-9ef8-6bae81375979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import jsondiff\n",
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "from datetime import date, datetime, timezone\n",
    "import csv\n",
    "import time\n",
    "\n",
    "with open('./config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "db_file = './app/patron_validation_data.db'\n",
    "\n",
    "class SQLiteConnection:\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "    with SQLiteConnection('your_database_path.sqlite') as conn:\n",
    "        # Your database operations here\n",
    "    \"\"\"\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_diff(new_json, old_json):\n",
    "        try:\n",
    "            diff_result = jsondiff.diff(\n",
    "                new_json, \n",
    "                old_json, \n",
    "                load=True, \n",
    "                dump=True, \n",
    "                marshal=True, \n",
    "                syntax='explicit'\n",
    "            )\n",
    "\n",
    "            # return an empty json array if the result of the diff is None, \n",
    "            # ... or the json result of the diff \n",
    "            return str(diff_result) if diff_result is not None else str('{}')\n",
    "\n",
    "        except Exception as e:\n",
    "            return str('{}')\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "        self.conn.create_function('json_diff', 2, self.compute_diff)\n",
    "        # run this sql script when the connection to the database is opened ... \n",
    "        sql = \"\"\"\\\n",
    "        PRAGMA locking_mode=SHARED;    -- https://www.sqlite.org/lockingv3.html\n",
    "        PRAGMA cache_size = 12800;     -- results in about 50MB of cache at the default 4KB page size\n",
    "        -- PRAGMA foreign_keys = ON;   -- TODO consider turning this on, and then running a check ...       \n",
    "        -- run a check ...\n",
    "        -- PRAGMA foreign_key_check;\n",
    "        \"\"\"\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.executescript(sql)\n",
    "        self.cursor.close()\n",
    "        del(self.cursor)\n",
    "        return self.conn\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # https://www.sqlite.org/pragma.html#pragma_analysis_limit\n",
    "        sql = \"\"\"\\\n",
    "        PRAGMA analysis_limit=-1;   -- no analysis limit\n",
    "        PRAGMA optimize;            -- analyze \n",
    "        PRAGMA analysis_limit=2000; -- set analysis limit back to something more reasonable \n",
    "        \"\"\"\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.executescript(sql)\n",
    "        \n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c3c594-3035-4443-bda2-e88b60a262a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up table schemas, indexes, and triggers\n",
    "with SQLiteConnection(db_file) as con:\n",
    "    cursor = con.cursor()\n",
    "\n",
    "    with open('./sql_schema.sql') as f:\n",
    "        cursor.executescript(f.read())\n",
    "\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9074583-aab1-48ef-8292-2f9a488dd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_patron_data(con, data, columns):\n",
    "    \"\"\"\n",
    "    takes the patron data returned from the Sierra SQL query, and inserts it into the local database\n",
    "    \n",
    "    con            : sqlite connection,\n",
    "    data           : data from the sierra result query\n",
    "    columns        : the column names\n",
    "    \"\"\"\n",
    "\n",
    "    sql_patron_insert = \"\"\"\\\n",
    "    INSERT INTO patrons (\n",
    "        patron_record_id, \n",
    "        patron_record_num, \n",
    "        campus_code, \n",
    "        barcode1, \n",
    "        home_library_code,\n",
    "        ptype_code, \n",
    "        create_timestamp_utc, \n",
    "        delete_timestamp_utc, \n",
    "        update_timestamp_utc, \n",
    "        expire_timestamp_utc, \n",
    "        active_timestamp_utc,\n",
    "        claims_returned_total, \n",
    "        owed_amt_cents, \n",
    "        mblock_code, \n",
    "        highest_level_overdue_num, \n",
    "        num_revisions\n",
    "    ) \n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ON CONFLICT(patron_record_id) DO UPDATE SET\n",
    "        patron_record_num = excluded.patron_record_num,\n",
    "        campus_code = excluded.campus_code,\n",
    "        barcode1 = excluded.barcode1,\n",
    "        home_library_code = excluded.home_library_code,\n",
    "        ptype_code = excluded.ptype_code,\n",
    "        create_timestamp_utc = excluded.create_timestamp_utc,\n",
    "        delete_timestamp_utc = excluded.delete_timestamp_utc,\n",
    "        update_timestamp_utc = excluded.update_timestamp_utc,\n",
    "        expire_timestamp_utc = excluded.expire_timestamp_utc,\n",
    "        active_timestamp_utc = excluded.active_timestamp_utc,\n",
    "        claims_returned_total = excluded.claims_returned_total,\n",
    "        owed_amt_cents = excluded.owed_amt_cents,\n",
    "        mblock_code = excluded.mblock_code,\n",
    "        highest_level_overdue_num = excluded.highest_level_overdue_num,\n",
    "        num_revisions = excluded.num_revisions\n",
    "    ;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        con.executemany(\n",
    "            sql_patron_insert, \n",
    "            (\n",
    "                (\n",
    "                    int(row[columns.index('patron_record_id')]) \\\n",
    "                        if row[columns.index('patron_record_id')] is not None else None,\n",
    "                    int(row[columns.index('patron_record_num')]) \\\n",
    "                        if row[columns.index('patron_record_num')] is not None else None,\n",
    "                    str(row[columns.index('campus_code')]) \\\n",
    "                        if row[columns.index('campus_code')] is not None else None,\n",
    "                    str(row[columns.index('barcode1')]) \\\n",
    "                        if row[columns.index('barcode1')] is not None else None,\n",
    "                    str(row[columns.index('home_library_code')]) \\\n",
    "                        if row[columns.index('home_library_code')] is not None else None,\n",
    "                    str(row[columns.index('ptype_code')]) \\\n",
    "                        if row[columns.index('ptype_code')] is not None else None,\n",
    "                    int(row[columns.index('create_timestamp_utc')]) \\\n",
    "                        if row[columns.index('create_timestamp_utc')] is not None else None,\n",
    "                    int(row[columns.index('delete_timestamp_utc')]) \\\n",
    "                        if row[columns.index('delete_timestamp_utc')] is not None else None,\n",
    "                    int(row[columns.index('update_timestamp_utc')]) \\\n",
    "                        if row[columns.index('update_timestamp_utc')] is not None else None,\n",
    "                    int(row[columns.index('expire_timestamp_utc')]) \\\n",
    "                        if row[columns.index('expire_timestamp_utc')] is not None else None,\n",
    "                    int(row[columns.index('active_timestamp_utc')]) \\\n",
    "                        if row[columns.index('active_timestamp_utc')] is not None else None,\n",
    "                    int(row[columns.index('claims_returned_total')]) \\\n",
    "                        if row[columns.index('claims_returned_total')] is not None else None,\n",
    "                    int(row[columns.index('owed_amt_cents')]) \\\n",
    "                        if row[columns.index('owed_amt_cents')] is not None else None,\n",
    "                    str(row[columns.index('mblock_code')]) \\\n",
    "                        if row[columns.index('mblock_code')] is not None else None,\n",
    "                    int(row[columns.index('highest_level_overdue_num')]) \\\n",
    "                        if row[columns.index('highest_level_overdue_num')] is not None else None,\n",
    "                    int(row[columns.index('num_revisions')]) \\\n",
    "                        if row[columns.index('num_revisions')] is not None else None,\n",
    "                )\n",
    "                for row in data\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# insert json_data function ... \n",
    "def insert_json_data(con, data, columns, json_data_type):\n",
    "    \"\"\"\n",
    "    con            : sqlite connection,\n",
    "    data           : data from the sierra result query\n",
    "    columns        : the column names\n",
    "    json_data_type : the type of data\n",
    "    ---\n",
    "    json_data_type come from the Sierra SQL results (columns)\n",
    "        patron_address_json,\n",
    "        identifiers_json,\n",
    "        phone_numbers_json,\n",
    "        emails_json\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    INSERT INTO patron_json_data (\n",
    "        patron_record_id,\n",
    "        json_data_type,\n",
    "        json_data\n",
    "    ) VALUES (\n",
    "        ?,\n",
    "        ?,\n",
    "        CASE WHEN json_valid(?) THEN ? ELSE '{}' END\n",
    "    )\n",
    "    ON CONFLICT(patron_record_id, json_data_type) DO UPDATE SET\n",
    "        json_data = CASE WHEN json_valid(excluded.json_data) THEN excluded.json_data ELSE '{}' END,\n",
    "        update_timestamp_utc = strftime('%s', 'now')\n",
    "    ;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        con.executemany(\n",
    "            sql,\n",
    "            (\n",
    "                (\n",
    "                    int(row[columns.index('patron_record_id')]),  # patron_record_id\n",
    "                    str(json_data_type),                          # json_data_type\n",
    "                    str(row[columns.index(json_data_type)]),      # json_data\n",
    "                    str(row[columns.index(json_data_type)]),      # json_data\n",
    "                    # NOTE, we need to send the json_data 2x since it's referenced 2x in the statement\n",
    "                )\n",
    "                for row in data\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'{json_data_type} Error:', e)\n",
    "\n",
    "\n",
    "def get_latest_update(cursor):\n",
    "    \"\"\"\n",
    "    extract the latest UNIX timestamp from the sqlite database\n",
    "    \"\"\"\n",
    "    \n",
    "    latest_update = 0 # 0 is the start time of UNIX EPOCH\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    SELECT\n",
    "        -- grab the last recent change we made, \n",
    "        max(patrons.update_timestamp_utc)\n",
    "        \n",
    "        -- if we want to buffer it by 10 minutes, use this\n",
    "        -- max(patrons.update_timestamp_utc) - 600\n",
    "    FROM\n",
    "    \tpatrons\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        result = cursor_sqlite.fetchone()\n",
    "        if (\n",
    "            len(result) == 1\n",
    "            and result[0] is not None\n",
    "        ):\n",
    "            latest_update = str(result[0])\n",
    "\n",
    "        # print(latest_update, '\\n')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return(0)\n",
    "\n",
    "    return(latest_update)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1103b0c-3646-41de-b428-5a44e2884ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692032601,.1692032745,.1692032751,.1692032761,.1692032769,.1692032777,.1692032785,.1692032793,.1692032801,.1692032809,.1692032816,.1692032826,.1692032834,.1692032842,.1692032849,.1692032857,.1692032864,.1692032873,.1692032881,.1692032888,.1692032896,.1692032904,.1692032912,.1692032920,.1692032928,.1692032935,.1692032943,.1692032951,.1692032959,.1692032967,.1692032975,.1692032982,.1692032990,.1692032998,.1692033006,.1692033014,.1692033022,.1692033029,.1692033037,.1692033045,.1692033054,.1692033062,.1692033070,.1692033077,.1692033086,.1692033094,.1692033101,.1692033109,.1692033117,.1692033125,.1692033133,.1692033140,.1692033148,.1692033156,.1692033163,.1692033172,.1692033179,.1692033187,.1692033195,.1692033203,.1692033211,.1692033218,.1692033226,.1692033234,.1692033242,.1692033250,.1692033259,.1692033265,.1692033273,.1692033282,.1692033290,.1692033298,.1692033306,.1692033314,.1692033322,.1692033330,.1692033338,.1692033345,.1692033351,.1692033362,.1692033370,.1692033377,.1692033385,.1692033394,.1692033401,.1692033409,.1692033417,.1692033425,.1692033433,.1692033441,.1692033449,.1692033457,.1692033465,.1692033473,.1692033481,.1692033488,.1692033496,.1692033504,.1692033513,.1692033521,.1692033527,.1692033536,.1692033544,.1692033552,.1692033560,.1692033567,.1692033573,.1692033583,.1692033591,.1692033599,.1692033607,.1692033615,.1692033623,.1692033631,.1692033639,.1692033647,.1692033655,.1692033663,.1692033671,.1692033679,.1692033687,.1692033695,.1692033703,.1692033710,.1692033719,.1692033727,.1692033734,.1692033742,.1692033749,.1692033759,.1692033767,.1692033775,.1692033783,.1692033791,.1692033799,.1692033806,.1692033815,.1692033823,.1692033831,.1692033839,.1692033846,.1692033855,.1692033863,.1692033870,.1692033879,.1692033887,.1692033894,.1692033903,.1692033910,.1692033918,.1692033927,.1692033936,.1692033944,.1692033953,.1692033961,.1692033970,.1692033978,.1692033987,.1692033994,.1692034003,.1692034011,.1692034018,.1692034026,.1692034034,.1692034042,.1692034050,.1692034058,.1692034066,.1692034074,.1692034082,.1692034090,.1692034098,.1692034106,.1692034114,.1692034122,.1692034130,.1692034138,.1692034146,.1692034154,.1692034162,.1692034169,.1692034177,.1692034186,.1692034194,.1692034202,.1692034208,.1692034217,.1692034226,.1692034233,.1692034241,.1692034249,.1692034257,.1692034265,.1692034274,.1692034280,.1692034289,.1692034297,.1692034305,.1692034313,.1692034321,.1692034329,.1692034337,.1692034345,.1692034353,.1692034361,.1692034369,.1692034377,.1692034385,.1692034393,.1692034401,.1692034407,.1692034417,.1692034425,."
     ]
    }
   ],
   "source": [
    "sql_patron_insert = \"\"\"\\\n",
    "INSERT INTO patrons (\n",
    "    patron_record_id, \n",
    "    patron_record_num, \n",
    "    campus_code, \n",
    "    barcode1, \n",
    "    home_library_code,\n",
    "    ptype_code, \n",
    "    create_timestamp_utc, \n",
    "    delete_timestamp_utc, \n",
    "    update_timestamp_utc, \n",
    "    expire_timestamp_utc, \n",
    "    active_timestamp_utc,\n",
    "    claims_returned_total, \n",
    "    owed_amt_cents, \n",
    "    mblock_code, \n",
    "    highest_level_overdue_num, \n",
    "    num_revisions\n",
    ") \n",
    "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "ON CONFLICT(patron_record_id) DO UPDATE SET\n",
    "    patron_record_num = excluded.patron_record_num,\n",
    "    campus_code = excluded.campus_code,\n",
    "    barcode1 = excluded.barcode1,\n",
    "    home_library_code = excluded.home_library_code,\n",
    "    ptype_code = excluded.ptype_code,\n",
    "    create_timestamp_utc = excluded.create_timestamp_utc,\n",
    "    delete_timestamp_utc = excluded.delete_timestamp_utc,\n",
    "    update_timestamp_utc = excluded.update_timestamp_utc,\n",
    "    expire_timestamp_utc = excluded.expire_timestamp_utc,\n",
    "    active_timestamp_utc = excluded.active_timestamp_utc,\n",
    "    claims_returned_total = excluded.claims_returned_total,\n",
    "    owed_amt_cents = excluded.owed_amt_cents,\n",
    "    mblock_code = excluded.mblock_code,\n",
    "    highest_level_overdue_num = excluded.highest_level_overdue_num,\n",
    "    num_revisions = excluded.num_revisions\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "# connect to both the sierra db, and the local sqlite db\n",
    "with psycopg2.connect(dsn=config['dsn']) as con, \\\n",
    "SQLiteConnection(db_file) as con_sqlite:\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + 60*60  # 60 minutes * 60 seconds/minute\n",
    "    # end_time = start_time + 5*60\n",
    "\n",
    "    while time.time() < end_time:\n",
    "        cursor = con.cursor(name=\"named_cursor\")\n",
    "        cursor_sqlite = con_sqlite.cursor()\n",
    "    \n",
    "        # get the latest UNIX timestamp from the local database\n",
    "        latest_update = get_latest_update(cursor_sqlite)\n",
    "        print(latest_update, end=',')\n",
    "    \n",
    "        try:\n",
    "            with open('./sierra_patron_data.sql') as f:\n",
    "                cursor.execute(f.read(), (latest_update,))\n",
    "        except Exception as e:\n",
    "            print('Error executing sierra_patron_data.sql', e)\n",
    "            \n",
    "        i = 0\n",
    "        try:\n",
    "            while(data:=cursor.fetchmany(1000)):\n",
    "                # get the columns from the cursor\n",
    "                columns = [col[0] for col in cursor.description]\n",
    "                \n",
    "                # insert the appropriate data into each local sqlite table ..\n",
    "                # patrons table\n",
    "                insert_patron_data(con_sqlite, data, columns)\n",
    "    \n",
    "                # insert the json data\n",
    "                # (NOTE: `json_data_type` is the row's column data from the Sierra \n",
    "                # SQL query that contains the JSON data of that type \n",
    "                insert_json_data(con_sqlite, data, columns, 'patron_address_json')\n",
    "                insert_json_data(con_sqlite, data, columns, 'identifiers_json')\n",
    "                insert_json_data(con_sqlite, data, columns, 'phone_numbers_json')\n",
    "                insert_json_data(con_sqlite, data, columns, 'emails_json')\n",
    "                \n",
    "                i+=1\n",
    "                print('.', end='')\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "              \n",
    "        # print('done. ', i)\n",
    "        con_sqlite.commit()\n",
    "        cursor_sqlite.close()\n",
    "        cursor.close()\n",
    "        del(cursor_sqlite)\n",
    "        del(cursor)\n",
    "        time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
